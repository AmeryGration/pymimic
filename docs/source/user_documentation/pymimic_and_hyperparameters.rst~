.. _pymimic_and_optimization:

Hyperparameter optimization and PyMimic
=======================================

The optimization of the covariance function hyperparameter is performed
automatically by the :func:`pymimic.gpe`. The optimized hyperparameter is
returned as the ``hyp`` attribute of the ``GpeResult`` object. However, the
hyperparameter may also be optimized using :func:`pymimic.hyp_opt`. We may
choose between the maximum-likelihood and maximum-pseudolikelihood method
using the keyword ``'opttype'``, which takes the values ``'likelihood'`` and
``'loo'``.

Let us optimize the hyperparameter of the squared-exponential covariance
function using this method. First generate the training data.

.. sourcecode:: python

   >>> import pymimic as mim
   >>> bounds = ((-5., 10.), (0., 15.))
   >>> xtrain = mim.design(bounds)
   >>> ytrain = [mim.testfunc.branin(xi) for xi in xtrain]

Now perform the optimization.

.. sourcecode:: python

   >>> mim.hyp_opt(xtrain, ytrain, covfunc=mim.covfunc.se)
   [31036.294, 0.0689548136, 0.00520141501]

The optimization of the hyperparameters is by far the most computationally
expensive aspect of GPE. Consequently :func:`pymimic.hyp_opt` may take some
time (say, several minutes for a low-dimensional function) to terminate
successfully.

The function :func:`pymimic.gpe` optimizes the hyperparameters using
:func:`pymimic.hyp_opt`. It possible to override this automatic optimization
by passing the hyperparameters directly using the keyword ``hyp``.

.. sourcecode:: python

   >>> hyp = _
   >>> x = mim.design(bounds, 'random', 1)
   >>> result = mim.gpe(x, xtrain, ytrain, hyp=hyp)
   >>> result.y
   array([ 26.62129408])

It is good practice to reuse the hyperparameter value in this way as it avoids
our having to optimize the parameter unnecessarily.

The function :func:`pymimimc.hyp_opt` only works for covariance functions
contained in the subpackage ``pymimic.covfunc``. The hyperparameters of
user-specified covariance function must be optimized manually. This can be
done my maximizing either of the functions :func:`pymimic.log_likelihood` or
:func:`pymimic.log_loo_likelihood`.


Maximization algorithm
----------------------

The function :func:`pymimic.hyp_opt` optimizes a covariance function's
hyperparameter by maximizing either :math:`\ln L` or :math:`\ln
L_\mathrm{LOO}` using the multistart method, initialized with a Latin
hypersquare design of size :math:`10D + 1`. This multistart optimization is
implemented by :func:`pymimic.multistart`.

In order to perform the maximimization we must decide upon some feasible
region of hyperparameter space in which to search. These are specific to a
chosen covariance function model. Consider the optimization of the
hyperparameter of the squared-exponential covariance function, recalling that
the elements of the metric matrix, :math:`\boldsymbol{M}`, define a vector of
length scales, :math:`\boldsymbol{L} = \operatorname{diag}(m_1^{-1/2},
m_2^{-1/2}, \dots , m_n^{-1/2})`. If we are to avoid overfitting these length
scales may not be smaller that the separation of design points. If we have
used a Latin-hypersquare design of size :math:`10D` the separation in the
:math:`i`-th coordinate is :math:`L_{i}/10` where :math:`L_{i}` is the
difference between the largest and smallest coordinate. If we are to avoid
underfitting we might expect the lenths scales to be no greater than
:math:`L_{i}`. We therefore search over a region of parameter space,
:math:`\boldsymbol{X} = \prod_{i = 1}^{D} [(L_i/10)^2, L_i^2]`. There are no
such bounds on the signal variance, :math:`\sigma^2`, which we know only to be
positive. We search over the positive real numbers mapped to the interval
:math:`[0, 2 \pi]` using the arctan transformation. If you wish to override
these bounds, you may do so using the keyword ``bounds`` in :func:`hyp_opt`.
